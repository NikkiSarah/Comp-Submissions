{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **April 2022 Tabular Playground Competition EDA**","metadata":{}},{"cell_type":"markdown","source":"* [Setup / Preliminaries](#setup)\n* [Visual Exploration](#visualisations)\n* [Dimensionality Reduction](#dimred)\n    - [Original Data](#dimred1)\n    - [Pivoted Data](#dimred2)\n* [Generalised Additive Model](#gam)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"setup\"></a>\n### **Setup / Preliminaries**\n\nREADME: I'm commenting my code so you can hopefully follow my general thought process, but I haven't been motivated enough to include a full markdown commentary or polish the notebook.","metadata":{}},{"cell_type":"code","source":"# masking the warning messages\noptions(warn = -1)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:02:28.599354Z","iopub.execute_input":"2022-04-05T06:02:28.600725Z","iopub.status.idle":"2022-04-05T06:02:28.612390Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# import any necessary libraries\n# as per the (deleted) intro cell, the R environment is defined by the kaggle/rstats Docker image: https://github.com/kaggle/docker-rstats\nlibrary(repr)\nlibrary(tidyverse)\nlibrary(corrplot)\nlibrary(GGally)\nlibrary(tidymodels)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(mgcv)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:02:28.790428Z","iopub.execute_input":"2022-04-05T06:02:28.791671Z","iopub.status.idle":"2022-04-05T06:02:32.342614Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# read in the data\ntrain_data <- read_csv(\"../input/tabular-playground-series-apr-2022/train.csv\", show_col_types = FALSE)\ntrain_labels <- read_csv(\"../input/tabular-playground-series-apr-2022/train_labels.csv\", show_col_types = FALSE)\ntest_data <- read_csv(\"../input/tabular-playground-series-apr-2022/test.csv\", show_col_types = FALSE)\nsample_submission <- read_csv(\"../input/tabular-playground-series-apr-2022/sample_submission.csv\", show_col_types = FALSE)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:02:32.344459Z","iopub.execute_input":"2022-04-05T06:02:32.345604Z","iopub.status.idle":"2022-04-05T06:02:34.770030Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Recall the files and field description from the competition's *Data* tab:\n- train.csv - the training set, comprising ~26,000 60-second recordings of thirteen biological sensors for almost one thousand experimental participants\n    - sequence - a unique id for each sequence\n    - subject - a unique id for the subject in the experiment\n    - step - time step of the recording, in one second intervals\n    - sensor_00 - sensor_12 - the value for each of the thirteen sensors at that time step\n\n- train_labels.csv - the class label for each sequence.\n    - sequence - the unique id for each sequence.\n    - state - the state associated to each sequence. This is the target which you are trying to predict.","metadata":{}},{"cell_type":"code","source":"# look at the last couple of rows for each file\nprint(\"Training data\")\ntail(train_data)\nprint(\"Training labels\")\ntail(train_labels)\nprint(\"Testing data\")\ntail(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:02:34.772000Z","iopub.execute_input":"2022-04-05T06:02:34.773149Z","iopub.status.idle":"2022-04-05T06:02:34.852984Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# check out the dimensions\ndim(train_data)\ndim(train_labels)\ndim(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:02:34.856469Z","iopub.execute_input":"2022-04-05T06:02:34.858331Z","iopub.status.idle":"2022-04-05T06:02:34.893591Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# and major summary statistics for the training data\nsummary(train_data)\nsummary(train_labels)","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:02:34.896571Z","iopub.execute_input":"2022-04-05T06:02:34.898380Z","iopub.status.idle":"2022-04-05T06:02:58.298757Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# confirm that there are no missing values\nsum(is.na(train_data))","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:02:58.300616Z","iopub.execute_input":"2022-04-05T06:02:58.301667Z","iopub.status.idle":"2022-04-05T06:03:03.152756Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# check if there is an unbalanced target in the training set\ntable(train_labels$state)\n\noptions(repr.plot.width = 8, repr.plot.height = 7)\n\ntrain_labels %>%\n    ggplot(aes(x = state)) +\n    geom_bar(fill = \"chartreuse4\") +\n    scale_x_discrete(limits = c(0, 1), breaks = c(0, 1)) +\n    labs(x = \"State\",\n         y = \"Count\") +\n    theme_classic()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:03:03.154524Z","iopub.execute_input":"2022-04-05T06:03:03.155555Z","iopub.status.idle":"2022-04-05T06:03:03.639795Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"visualisations\"></a>\n### **Visual Exploration**","metadata":{}},{"cell_type":"code","source":"# check out the distribution of sequences per subject\noptions(repr.plot.width = 14, repr.plot.height = 7)\n\ntrain_data %>%\n    group_by(subject) %>%\n    distinct(sequence) %>%\n    count() %>%\n    ggplot(aes(x = n)) +\n    geom_histogram(binwidth = 1, fill = \"chartreuse4\", col = \"steelblue\") +\n    labs(x = \"Number of sequences\",\n         y = \"Number of subjects\") +\n    theme_classic() +\n    theme(text = element_text(size = 15))\n\n# ... and per subject per state\ntrain_data_incl_target <- left_join(train_data, train_labels, by = \"sequence\")\n\ntrain_data_incl_target %>%\n    group_by(subject, state) %>%\n    distinct(sequence) %>%\n    count() %>%\n    ggplot(aes(x = n, fill = factor(state))) +\n    geom_histogram(binwidth = 1, col = \"steelblue\") +\n    labs(x = \"Number of sequences\",\n         y = \"Number of subjects\") +\n    scale_fill_brewer(palette = \"Greens\", name = \"State\") +\n    theme_classic() +\n    theme(text = element_text(size = 15))","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:03:03.642045Z","iopub.execute_input":"2022-04-05T06:03:03.643222Z","iopub.status.idle":"2022-04-05T06:03:05.752632Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Now that's interesting. The single plot of the number of sequences for all subjects was approximately normally distributed, albeit with a long right tail. However, when grouped by state, we see that this is reflected only in the distibution of those recording state 0. The distribution of state 1 is not at all normal.","metadata":{}},{"cell_type":"code","source":"# examine the distributions of each of the sensors, including broken up by State\nlong_train_data <- train_data_incl_target %>%\n    pivot_longer(cols = starts_with(\"sensor\"),\n                names_to = \"sensor\",\n                values_to = \"reading\")\n\noptions(repr.plot.width = 25, repr.plot.height = 15)\n\n## original data with extreme values not shown\nlong_train_data %>%\n    ggplot(aes(x = reading)) +\n    geom_histogram(aes(y = ..density..), bins = 50, fill = \"chartreuse4\", col = \"black\") +\n    facet_wrap(vars(sensor), ncol = 3, scales = \"free_y\") +\n    xlim(-5, 5) +\n    labs(x = \"Number of sequences\",\n         y = \"Number of subjects\") +\n    theme_classic()\n\noptions(repr.plot.width = 25, repr.plot.height = 15)\n\n## and grouped by State\nlong_train_data %>%\n    ggplot(aes(x = reading, fill = factor(state))) +\n    geom_histogram(aes(y = ..density..), bins = 50, col = \"steelblue\") +\n    facet_wrap(vars(sensor), ncol = 3, scales = \"free_y\") +\n    xlim(-5, 5) +\n    labs(x = \"Number of sequences\",\n         y = \"Number of subjects\") +\n    scale_fill_brewer(palette = \"Greens\", name = \"State\") +\n    theme_classic()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T06:03:05.754513Z","iopub.execute_input":"2022-04-05T06:03:05.755625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for any patterns in the aggregate statistics of sensor readings for each step\noptions(repr.plot.width = 15, repr.plot.height = 7)\n\nlong_train_data %>%\ngroup_by(step, state) %>%\nsummarise(mean_reading = mean(reading),\n          median_reading = median(reading),\n          q25_reading = quantile(reading, probs = 0.25),\n          q75_reading = quantile(reading, probs = 0.75)) %>%\n    ggplot(aes(x = step, y = mean_reading, col = factor(state))) +\n    geom_line() +\n    geom_hline(yintercept = 0, linetype = 'dashed') +\n    facet_wrap(vars(state)) +\n    labs(x = \"Step\",\n         y = \"Average sensor reading\") +\n    scale_x_continuous(breaks = seq(0, 60, 5)) +\n    scale_colour_brewer(palette = \"Set1\", name = \"State\") +\n    theme_classic() +\n    theme(legend.position = \"none\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, it's quite interesting that the average value of the sensors is lower for State 0 even though both bounce around quite a bit during the 60 second period.","metadata":{}},{"cell_type":"code","source":"# and examine the readings for each state faceted by sensor\noptions(repr.plot.width = 25, repr.plot.height = 15)\n\nlong_train_data %>%\ngroup_by(step, state, sensor) %>%\nsummarise(mean_reading = mean(reading),\n          median_reading = median(reading),\n          q25_reading = quantile(reading, probs = 0.25),\n          q75_reading = quantile(reading, probs = 0.75)) %>%\n    ggplot(aes(x = step, y = mean_reading, col = factor(state))) +\n    geom_line() +\n    geom_hline(yintercept = 0, linetype = 'dashed') +\n    facet_wrap(vars(sensor), scales = \"free_y\") +\n    labs(x = \"Step\",\n         y = \"Average sensor reading\") +\n    scale_x_continuous(breaks = seq(0, 60, 10)) +\n    scale_colour_brewer(palette = \"Set1\", name = \"State\") +\n    theme_classic()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The biggest insight here is probably that the difference in average values by state is possibly driven by sensor 2 and to a lessor extent, sensor 4. Also, sensors 4, 10 and 12 tend to bounce around a little less than the others (excluding sensor 2 which appears to behave quite differently than the others).","metadata":{}},{"cell_type":"code","source":"# visualise the linear correlations\n# using spearman instead of the default pearson as it's unlikely that the data come from a bivariate normal distribution\ncorr_data <- train_data_incl_target %>% \n    dplyr::select(-c(sequence, subject))\n\noptions(repr.plot.width = 10, repr.plot.height = 10)\n\ncorr_mat <- round(cor(corr_data, method = \"spearman\"), 4)\ncorrplot::corrplot(corr_mat, method = \"ellipse\", type = \"upper\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a closer look at the relationships between features with the strongest correlations\noptions(repr.plot.width = 25, repr.plot.height = 15)\n\ncorr_data <- train_data_incl_target %>%\n    dplyr::select(c(sensor_00, sensor_01, sensor_03, sensor_06, sensor_07, sensor_09, sensor_11, state))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"options(repr.plot.width = 10, repr.plot.height = 7)\n\ncorr_data %>%\n    ggplot(aes(x = sensor_00, y = sensor_06, col = factor(state))) +\n    geom_jitter() +\n    scale_colour_brewer(palette = \"Set1\", name = \"State\") +\n    theme_classic()\n\ncorr_data %>%\n    ggplot(aes(x = sensor_01, y = sensor_06, col = factor(state))) +\n    geom_jitter() +\n    scale_colour_brewer(palette = \"Set1\", name = \"State\") +\n    theme_classic()\n\ncorr_data %>%\n    ggplot(aes(x = sensor_09, y = sensor_06, col = factor(state))) +\n    geom_jitter() +\n    scale_colour_brewer(palette = \"Set1\", name = \"State\") +\n    theme_classic()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_data %>%\n    ggplot(aes(x = sensor_07, y = sensor_03, col = factor(state))) +\n    geom_jitter() +\n    scale_colour_brewer(palette = \"Set1\", name = \"State\") +\n    theme_classic()\n\ncorr_data %>%\n    ggplot(aes(x = sensor_11, y = sensor_03, col = factor(state))) +\n    geom_jitter() +\n    scale_colour_brewer(palette = \"Set1\", name = \"State\") +\n    theme_classic()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This doesn't seem to tell us much, except that state 1 values tend to be clustered more tightly together and state 0 is more likely to contain extreme values.","metadata":{}},{"cell_type":"code","source":"# as there were multiple rows for each sequence-subject combination, let's pivot the data to see what happens to the correlations\n# this means that there will only be a single row for each sequence-subject combination\nwide_train_data_incl_target <- train_data_incl_target %>%\n    pivot_longer(!c(sequence, subject, step, state), names_to = \"name\", values_to = \"value\") %>%\n    pivot_wider(names_from = c(step, name), values_from = value) %>%\n    arrange(sequence, subject)\n\nhead(wide_train_data_incl_target)\n# note that the column names are of the format [step]_[sensor]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_corr_data <- wide_train_data_incl_target\ncorr_mat <- round(cor(new_corr_data, method = \"spearman\"), 4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# turn the matrix into a dataframe and filter out any correlations less then 0.5\ncorr_mat_df <- as.data.frame(corr_mat) %>%\n    mutate(row_name = rownames(corr_mat)) %>%\n    pivot_longer(!row_name) %>%\n    rename(\n        col_name = name,\n        correlation = value\n    ) %>%\n    filter(correlation >= 0.5) %>%\n    arrange(desc(correlation))\n\n# also remove any correlations where the feature is compared against itself\ncorr_mat_df <- corr_mat_df[corr_mat_df$row_name != corr_mat_df$col_name, ]\nhead(corr_mat_df)\ndim(corr_mat_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of moderate to highly-correlated features in this pivoted dataframe suggests that dimensionality reduction could be useful. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"dimred\"></a>\n### **Dimensionality Reduction**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"dimred1\"></a>\n#### **Original Data**","metadata":{}},{"cell_type":"code","source":"# split the data, ensuring there are approximately equal numbers of each step in each (the dataset is big enough that it's probably reasonable to assume we'll get\n# roughly equal proportions of each state)\nset.seed(22)\ndata_split <- initial_split(train_data_incl_target[ , 3:17], prop = 0.75, strata = step)\ntrain_data <- training(data_split)\nval_data <- testing(data_split)\n\n# scale the input features\ntrain_data_sc <- as.data.frame(scale(train_data[-15]))\n#train_data_sc$state <- train_data$state","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_mod <- PCA(train_data[, 1:14], scale.unit = TRUE, ncp = 10, graph = FALSE)\n\neig_val <- get_eigenvalue(pca_mod)\n(eig_val)\nfviz_eig(pca_mod, addlabels = TRUE, ylim = c(0, 20))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first five PCs explain only 53% of the variance, but if we include all the PCs that have an eigenvalue of just below 1 (i.e. nine), we explain 74% of the variance. This is acceptable.","metadata":{}},{"cell_type":"code","source":"options(repr.plot.width = 10, repr.plot.height = 7)\nfviz_pca_var(pca_mod, col.var = \"cos2\", repel = TRUE)\nfviz_pca_var(pca_mod, col.var = \"contrib\", repel = TRUE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fviz_pca_ind(pca_mod, label = \"none\", habillage = train_data$state, select.ind = list(cos2 = 100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These plots basically indicate the importance of features to the first two PCs. Basically, only 7 out of the 13 features are important.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"gam\"></a>\n### **Generalised Additive Model**","metadata":{}},{"cell_type":"code","source":"# http://www.sthda.com/english/wiki/wiki.php?id_contents=7851\n# https://m-clark.github.io/generalized-additive-models/application.html","metadata":{},"execution_count":null,"outputs":[]}]}